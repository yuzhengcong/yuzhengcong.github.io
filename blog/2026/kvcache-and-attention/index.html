<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Attention Mechanisms: From MHA to DeepSeek's MLA | Yuzheng Cong </title> <meta name="author" content="Yuzheng Cong"> <meta name="description" content="An in-depth analysis of the KV Cache mechanism and the technological evolution from Multi-Head Attention (MHA) to Grouped-Query Attention (GQA), and finally to DeepSeek's revolutionary Multi-Head Latent Attention (MLA)."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Playfair+Display:400,700|Roboto:300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?v=6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yuzhengcong.github.io/blog/2026/kvcache-and-attention/"> <script src="/assets/js/theme.js?v=48c9b5bd7f2e0605e39e579400e22553"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yuzheng</span> Cong </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main" style="max-width: 1400px;"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Attention Mechanisms: From MHA to DeepSeek's MLA</h1> <p class="post-meta"> Created on January 28, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/tag/kv-cache"> <i class="fa-solid fa-hashtag fa-sm"></i> KV Cache</a>   <a href="/blog/tag/attention"> <i class="fa-solid fa-hashtag fa-sm"></i> Attention</a>   <a href="/blog/tag/deepseek"> <i class="fa-solid fa-hashtag fa-sm"></i> DeepSeek</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> LLM</a>   <a href="/blog/tag/transformer"> <i class="fa-solid fa-hashtag fa-sm"></i> Transformer</a>   ·   <a href="/blog/category/ai-technology"> <i class="fa-solid fa-tag fa-sm"></i> AI Technology</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="revisiting-key-value-cache">Revisiting Key-Value Cache</h2> <p>1.1 Computational Logic and Redundancy in Autoregressive Generation</p> <p>The process of generating text in Transformer models is performed token by token. When predicting the $(t+1)$-th token, the model needs to perform attention calculations combining the current token with all context information from the previous $t$ tokens.</p> <p>In the original attention mechanism, for every position in the sequence, the Query vector ($Q$), Key vector ($K$), and Value vector ($V$) must be calculated. Without caching technology, the model would have to recalculate the $K$ and $V$ matrices for all historical tokens when generating each new token. This would cause inference complexity to grow quadratically with sequence length ($O(N^2)$).</p> <p>The introduction of KV Cache aims to transform this repetitive computation into storage space: once the $K$ and $V$ of historical tokens are calculated, they are stored in video memory (VRAM). Subsequent steps only need to calculate the $Q$, $K$, and $V$ for the current new token and concatenate them with the cached historical $K$ and $V$. This mechanism successfully reduces the computational complexity of the inference phase from quadratic to linear ($O(N)$), which is the cornerstone of realizing real-time interactive AI.</p> <h3 id="12-linear-expansion-of-memory-usage-and-the-memory-wall">1.2 Linear Expansion of Memory Usage and the “Memory Wall”</h3> <p>Although KV Cache alleviates the computational bottleneck, it shifts the pressure to memory bandwidth and capacity. The size of the KV Cache is directly proportional to the sequence length, model layers, number of attention heads, and head dimension. The calculation formula is typically expressed as:</p> \[Memory_{KVCache} = 2 \times BatchSize \times Layers \times Heads \times Dim \times SeqLength \times BytesPerParam\] <p>When processing long texts, the memory occupied by KV Cache often exceeds the model weights themselves. For example, when the context reaches 80,000 tokens, even for a medium-sized model, its KV Cache can occupy tens of GB of VRAM.</p> <p>When VRAM is full, the system is forced to trigger “CPU Spill” (offloading data to system RAM), causing a dramatic increase in data transfer latency over the PCIe bus. Generation speed can plummet from tens of tokens per second to single digits. This precipitous drop in performance is known as the “Memory Wall” phenomenon.</p> <h2 id="chapter-2-structural-evolution-of-attention-mechanisms-from-mha-to-gqa">Chapter 2: Structural Evolution of Attention Mechanisms: From MHA to GQA</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://shreyansh26.github.io/post/2025-11-08_multihead-latent-attention/images/mla-480.webp 480w,https://shreyansh26.github.io/post/2025-11-08_multihead-latent-attention/images/mla-800.webp 800w,https://shreyansh26.github.io/post/2025-11-08_multihead-latent-attention/images/mla-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="https://shreyansh26.github.io/post/2025-11-08_multihead-latent-attention/images/mla.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <h3 id="21-multi-head-attention-mha-accuracy-benchmark-and-memory-bottleneck">2.1 Multi-Head Attention (MHA): Accuracy Benchmark and Memory Bottleneck</h3> <p>Multi-Head Attention (MHA) is the cornerstone of the Transformer architecture. By slicing the model dimension into multiple independent heads, it allows the model to simultaneously attend to information from different subspaces at different positions. In MHA, each Query Head is equipped with a unique corresponding Key Head and Value Head.</p> <p>Although this design has an extremely high ceiling for capturing complex semantic relationships, during inference, all $K$ and $V$ vectors for every layer and every head must be fully cached. When scaling models to hundreds of billions of parameters or extending contexts to tens of thousands of tokens, the demand for memory bandwidth far exceeds the physical limits of GPU HBM (High Bandwidth Memory), leading to massive idling of computational units (ALUs) and creating the so-called “memory-bound” bottleneck.</p> <h3 id="22-multi-query-attention-mqa-extreme-bandwidth-optimization">2.2 Multi-Query Attention (MQA): Extreme Bandwidth Optimization</h3> <p>To thoroughly break the memory limit, Multi-Query Attention (MQA) adopts an extreme sharing strategy: letting all query heads share the same pair of Key and Value heads. This design can directly reduce the KV Cache memory usage during inference to $1/H$ of MHA (where $H$ is the number of heads).</p> <p>MQA greatly improves inference throughput and allows for larger batch sizes, but its limitations are also very obvious. Since all attention heads must reference the same key-value pairs, the model’s performance in capturing fine-grained semantic differences and long-range complex dependencies drops significantly, and it is also more prone to instability during training. Therefore, MQA is mostly applied in edge computing devices that are extremely sensitive to resources or in specific small models.</p> <h3 id="23-grouped-query-attention-gqa-the-art-of-engineering-trade-offs">2.3 Grouped-Query Attention (GQA): The Art of Engineering Trade-offs</h3> <p>Grouped-Query Attention (GQA) is considered the optimal balance point between MHA and MQA. GQA divides query heads into multiple groups, where query heads within each group share the same pair of key/value heads. For example, the Llama 3 70B model has 64 query heads, and every 8 query heads are grouped together to share one KV pair (i.e., GQA-8).</p> <p>The introduction of GQA brings the following key advantages:</p> <ul> <li> <strong>Significant Memory Reduction</strong>: Taking GQA-8 as an example, KV Cache usage is reduced by 87.5% compared to MHA.</li> <li> <strong>Throughput Improvement</strong>: Since the amount of data read from HBM is drastically reduced, inference speed can usually achieve a 1.5x to 2x improvement.</li> <li> <strong>Accuracy Retention</strong>: Studies show that appropriate grouping strategies can achieve inference efficiency close to MQA with almost no loss in model accuracy.</li> </ul> <p>Currently, GQA has become the standard configuration for mainstream large language models (such as Llama 2/3, Mistral, Qwen 2.5, etc.) and is a core technology for solving modern inference workloads.</p> <h2 id="chapter-3-multi-head-latent-attention-mla-deepseeks-revolutionary-innovation">Chapter 3: Multi-Head Latent Attention (MLA): DeepSeek’s Revolutionary Innovation</h2> <h3 id="31-core-logic-of-latent-space-compression">3.1 Core Logic of Latent Space Compression</h3> <p>Multi-Head Latent Attention (MLA) is one of the most breakthrough technologies introduced in models like DeepSeek-V2/V3. Unlike GQA, which compresses the cache by reducing the number of heads, MLA utilizes the principle of Low-Rank Factorization in linear algebra to compress the $K$ and $V$ vectors of all heads into a compact Latent Space.</p> <p>In MLA, input vectors are first projected into a latent vector $c_{KV}$ with a rank much smaller than the original model dimension. During the inference phase, the $K$ and $V$ for each head are no longer stored; instead, only this compressed latent vector is stored. When attention calculation is needed, the model dynamically expands the latent vector into the full-dimension keys and values required by each head through an Up-projection Matrix. This method allows DeepSeek models to maintain a very large number of attention heads (e.g., 128 heads) while reducing the memory overhead of KV Cache to about 4.3% to 6.7% of MHA.</p> <h3 id="32-decoupled-rotary-position-embedding-decoupled-rope">3.2 Decoupled Rotary Position Embedding (Decoupled RoPE)</h3> <p>Since Rotary Position Embedding (RoPE) is closely related to token position, it possesses position sensitivity, which mathematically conflicts with the linear transformation properties of low-rank compression. If RoPE is applied directly to the latent vector, the compressed information cannot be correctly deconstructed.</p> <p>DeepSeek’s solution is a “Decoupled Attention Structure”: it splits the $K$ and $Q$ vectors into two parts—one is the “content part” carrying semantic information, which applies low-rank compression; the other is the “position part” carrying spatial information, which applies RoPE independently. When calculating attention scores, the results of the content part and the position part are superimposed. This design not only overcomes the compatibility problem between RoPE and compression but also further optimizes KV Cache because only the latent vector of the content part and a small portion of the common position vector need to be cached.</p> <h3 id="33-matrix-absorption-and-computational-optimization-during-inference">3.3 Matrix Absorption and Computational Optimization During Inference</h3> <p>MLA’s advantage in inference efficiency is reflected not only in space but also in the optimization of computational logic. During inference, MLA allows for “Matrix Absorption”: multiple matrix multiplications (transpose of $Q$ and $K$) that originally needed to be performed can be recombined. Since the Up-projection Matrix is static and independent of the input, it can be pre-merged with the weight matrix. This further reduces the computational overhead of MLA during inference, supporting higher throughput autoregressive decoding.</p> <h2 id="summary-comparison-of-different-mechanisms">Summary: Comparison of Different Mechanisms</h2> <table> <thead> <tr> <th style="text-align: left">Feature</th> <th style="text-align: left">MHA</th> <th style="text-align: left">GQA</th> <th style="text-align: left">MLA (DeepSeek)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>KV Cache Compression Principle</strong></td> <td style="text-align: left">None (Full Storage)</td> <td style="text-align: left">Head Sharing (Reduced Heads)</td> <td style="text-align: left">Low-Rank Factorization (Dimension Compression)</td> </tr> <tr> <td style="text-align: left"><strong>KV Cache Size</strong></td> <td style="text-align: left">$2 \cdot H \cdot d_h$</td> <td style="text-align: left">$2 \cdot G \cdot d_h$</td> <td style="text-align: left">$d_c + d_R$ (Latent Dimension)</td> </tr> <tr> <td style="text-align: left"><strong>Accuracy Retention</strong></td> <td style="text-align: left">Best (Benchmark)</td> <td style="text-align: left">High (Slight Loss)</td> <td style="text-align: left">Very High (Better or Equal to MHA)</td> </tr> <tr> <td style="text-align: left"><strong>Context Support</strong></td> <td style="text-align: left">Limited</td> <td style="text-align: left">Medium</td> <td style="text-align: left">Very Strong (Supports 128K+)</td> </tr> </tbody> </table> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/plotly/">a post with plotly.js</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/photo-gallery/">a post with image galleries</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/tabs/">a post with tabs</a> </li> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script defer src="/assets/js/giscus-setup.js"></script> <noscript> Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Yuzheng Cong. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?v=c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>